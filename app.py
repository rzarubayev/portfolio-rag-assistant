import os
import sys
import importlib
import shutil
import requests
import logging 

import yaml
import gradio as gr
from langchain_core.document_loaders import BaseLoader
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain.prompts import PromptTemplate
import langchain_core.output_parsers as parsers
from langchain_google_genai import ChatGoogleGenerativeAI

# –ò–∑–º–µ–Ω–µ–Ω–∏–µ —É—Ä–æ–≤–Ω—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è unstructured
logging.getLogger("unstructured").setLevel(logging.ERROR)

# –ë–∞–∑–æ–≤—ã–π –∫–æ–Ω—Ñ–∏–≥ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    stream=sys.stdout
)

class PortfolioAssistant:
    """
    AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –Ω–∞ –±–∞–∑–µ RAG –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ GitHub-–ø–æ—Ä—Ç—Ñ–æ–ª–∏–æ.

    –≠—Ç–æ—Ç –∫–ª–∞—Å—Å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É, –∫–æ—Ç–æ—Ä–∞—è –∑–∞–≥—Ä—É–∂–∞–µ—Ç
    –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è, –∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç –∏—Ö –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
    –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ
    –Ω–∞–π–¥–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –£–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —á–µ—Ä–µ–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª.
    """

    def __init__(self, config_file="config.yaml"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç —ç–∫–∑–µ–º–ø–ª—è—Ä –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –ø–æ –ø–æ—Ä—Ç—Ñ–æ–ª–∏–æ.

        –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –≤—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–ª–Ω—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞:
        1.  –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –≤–∞–ª–∏–¥–∏—Ä—É–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª `config.yaml`.
        2.  –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –∫–ª–∞—Å—Å—ã –∑–∞–≥—Ä—É–∑—á–∏–∫–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (loaders).
        3.  –ü—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —Å–∫–∞—á–∏–≤–∞–µ—Ç —Ñ–∞–π–ª—ã –∏–∑ GitHub-—Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è.
        4.  –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏.
        5.  –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É ChromaDB.
        6.  –ó–∞–≥—Ä—É–∂–∞–µ—Ç LLM –∏ —Å–æ–∑–¥–∞–µ—Ç —Ü–µ–ø–æ—á–∫–∏ (chains) –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤.
        """

        logging.info("–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏...")
        if not os.path.exists(config_file):
            msg = "–§–∞–π–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–µ–≤–æ–∑–º–æ–∂–Ω–∞"
            self._raise_error(msg)
        else:
            with open(config_file, "r", encoding="utf-8") as f:
                self.config = yaml.safe_load(f)

        # –ü–æ–ª—É—á–µ–Ω–∏–µ –∑–∞–≥—Ä—É–∑—á–∏–∫–æ–≤ –∏ –∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        logging.info("–ò–º–ø–æ—Ä—Ç –∑–∞–≥—Ä—É–∑—á–∏–∫–æ–≤ –∏ –∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤...")
        msg = ""
        if "loaders" not in self.config: 
            msg = "–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∑–∞–≥—Ä—É–∑—á–∏–∫–∏ –≤ —Ñ–∞–π–ª–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏" 
        elif not isinstance(self.config["loaders"], dict):
            msg = ("–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Å–ø–∏—Å–∫–∞ –∑–∞–≥—Ä—É–∑—á–∏–∫–æ–≤, "
                   "–æ–∂–∏–¥–∞–µ—Ç—Å—è —Å–ª–æ–≤–∞—Ä—å '.ext': 'full.path.to.LoaderClass'")
        elif not self.config["loaders"]:
            msg = "–°–ø–∏—Å–æ–∫ –∑–∞–≥—Ä—É–∑—á–∏–∫–æ–≤ –ø—É—Å—Ç"
        if msg:
            self._raise_error(msg)
        self.loaders, self.loader_params = {}, {}
        # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –∏–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫ –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –∫–ª–∞—Å—Å–æ–≤
        for k, v in self.config["loaders"].items():
            self.loaders[k] = self._import_loader(v)
        if not isinstance(self.config["loader_params"], dict):
            logging.warning("–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –∑–∞–≥—Ä—É–∑—á–∏–∫–æ–≤")
        else:
            for k, v in self.config["loader_params"].items():
                self.loader_params[k] = v

        # –∑–∞–≥—Ä—É–∑–∫–∞ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        self.data_dir = self.config.get("data_dir", "data")
        self.chroma_dir = self.config.get("chroma_dir", "chroma")
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –ø–∞–ø–∫–∏ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
        if not os.path.exists(self.data_dir):
            logging.warning("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏, "
                            "–¥–∞–Ω–Ω—ã–µ –±—É–¥—É—Ç –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è GitHub")
            self._download_repo_files()
        general_docs, project_docs = self._load_documents()

        # –°–æ–∑–¥–∞–Ω–∏–µ –æ–±—â–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        if not general_docs:
            msg = "–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—â–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã"
            self._raise_error(msg)
        self.general_context = "\n\n".join(
            [doc.page_content for doc in general_docs]
        )
        # –°–æ–∑–¥–∞–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –ø–æ–µ–∫—Ç–æ–≤
        if not project_docs:
            msg = "–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ –ø—Ä–æ–µ–∫—Ç–∞–º"
            self._raise_error(msg)
        self.project_map = {
            doc.metadata["source"]: doc.page_content
            for doc in project_docs
        }

        # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        if "embedding_model" not in self.config:
            msg = "–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"
            self._raise_error(msg)
        self.embedding = HuggingFaceEmbeddings(
            model_name=self.config["embedding_model"])

        # –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã ChromaDB
        if os.path.exists(self.chroma_dir):
            self.vector_store = Chroma(
                persist_directory=self.chroma_dir,
                embedding_function=self.embedding
            )
            if self.vector_store._collection.count() > 0:
                is_new_store = False
            else:
                is_new_store = True
        else:
            is_new_store = True
        if is_new_store:
            logging.warning("–ë–∞–∑–∞ ChromaDB –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∏–ª–∏ –ø—É—Å—Ç–∞—è. "
                            "–ë—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω–∞ –Ω–æ–≤–∞—è –±–∞–∑–∞ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.")
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=self.config.get("chunk_size", 1000),
                chunk_overlap=self.config.get("chunk_overlap", 200)
            )
            chunks = text_splitter.split_documents(project_docs)
            logging.info(f"–î–æ–∫—É–º–µ–Ω—Ç—ã —Ä–∞–∑–¥–µ–ª–µ–Ω—ã –Ω–∞ {len(chunks)} —á–∞–Ω–∫–æ–≤. "
                         "–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑—ã ChromaDB...")
            self.vector_store = Chroma.from_documents(
                documents=chunks,
                embedding=self.embedding,
                persist_directory=self.chroma_dir
            )
            logging.info("–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑—ã ChromaDB –∑–∞–≤–µ—Ä—à–µ–Ω–æ.")
        logging.info("–ë–∞–∑–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–∞, —Å–æ–¥–µ—Ä–∂–∏—Ç "
                     f"{self.vector_store._collection.count()} –∑–∞–ø–∏—Å–µ–π.")
        # –ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞
        self.retriever = self.vector_store.as_retriever(
            search_type=self.config.get("search_type", "similarity"),
            search_kwargs=self.config.get("search_kwargs", {"k": 15})
        )
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ LLM
        if not os.environ.get("GOOGLE_API_KEY"):
            msg = "–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–∫—Ä—É–∂–µ–Ω–∏—è 'GOOGLE_API_KEY'"
            self._raise_error(msg)
        if "llm_model" not in self.config:
            msg = "–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ LLM –º–æ–¥–µ–ª–∏"
            self._raise_error(msg)
        try:
            self.llm = ChatGoogleGenerativeAI(
                model=self.config["llm_model"]
            )
            logging.info(f"–ú–æ–¥–µ–ª—å '{self.llm.model}' –∑–∞–≥—Ä—É–∂–µ–Ω–∞.")
        except Exception as e:
            msg = ("–í–æ–∑–Ω–∏–∫–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏ "
                   f"'{self.llm.model}': {e}")
            self._raise_error(msg)

        # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥–∞ –¥–ª—è –ª–∏–º–∏—Ç–æ–≤ –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É
        self.context_len = self.config.get("context_len", 100000)
        if not isinstance(self.context_len, int):
            msg = "–ó–Ω–∞—á–µ–Ω–∏–µ 'context_len' –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —Ü–µ–ª–æ—á–∏—Å–ª–µ–Ω–Ω—ã–º!"
            self._raise_error(msg)

        self.context_ratio = self.config.get("context_ratio", 2.5)
        if not isinstance(self.context_ratio, (int, float)):
            msg = "–ó–Ω–∞—á–µ–Ω–∏–µ 'context_ratio' –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —á–∏—Å–ª–µ–Ω–Ω—ã–º!"
            self._raise_error(msg)

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Ñ–æ—Ä–º–∞—Ç–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø—Ä–æ–º–ø—Ç–æ–≤
        msg = ""
        prompts = ["history_prompt", "research_prompt", "answer_prompt"]
        configs = {
            "template": str, 
            "input_variables": list, 
            "output_parser": str
        }
        for prompt in prompts:
            if prompt not in self.config:
                msg += f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–æ–Ω—Ñ–∏–≥ –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞ '{prompt}'. "
            elif not isinstance(self.config[prompt], dict):
                msg += (
                    f"–ö–æ–Ω—Ñ–∏–≥ –ø—Ä–æ–º–ø—Ç–∞ '{prompt}' –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–ª–æ–≤–∞—Ä–µ–º, "
                    f"–∏ —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–µ —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö {configs}"
                )
            else:
                current_prompt = self.config[prompt]
                for cfg_name, cfg_type in configs.items():
                    if cfg_name not in current_prompt:
                        msg += (
                            f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–æ–Ω—Ñ–∏–≥ '{cfg_name}' –¥–ª—è '{prompt}'. "
                        )
                    elif not isinstance(current_prompt[cfg_name], cfg_type):
                        msg += (
                            f"–ó–Ω–∞—á–µ–Ω–∏–µ '{cfg_name}' –Ω–µ —è–≤–ª—è–µ—Ç—Å—è '{cfg_type}'. "
                        )
                    elif not current_prompt[cfg_name]:
                        msg += (
                            f"–ó–Ω–∞—á–µ–Ω–∏–µ '{current_prompt[cfg_name]}' –ø—É—Å—Ç–æ–µ. ")
                    elif cfg_name == "output_parser":
                        parser_name = current_prompt[cfg_name]
                        try:
                            parser_class = getattr(parsers, parser_name)
                            if not issubclass(parser_class,
                                            parsers.BaseOutputParser):
                                msg += (
                                    f"–ü–∞—Ä—Å–µ—Ä '{parser_name} –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å "
                                    f"–¥–æ—á–µ—Ä–Ω–∏–º –∫–ª–∞—Å—Å–æ–º 'BaseOutputParser'. ")
                        except Exception as e:
                            msg += (
                                f"\n–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –ø–∞—Ä—Å–µ—Ä–∞ "
                                f"'{parser_name}': {e}\n")
        if msg:
            self._raise_error(msg)

        # –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ø–æ—á–µ–∫ –¥–ª—è –ø—Ä–æ–º–ø—Ç–æ–≤
        self.chains, self.inputs = {}, {}
        for prompt in prompts:
            current_prompt = self.config[prompt]
            prompt_template = PromptTemplate(
                template=current_prompt["template"],
                input_variables=current_prompt["input_variables"]
            )
            parser_class = getattr(parsers, current_prompt["output_parser"])
            ch_name = prompt.replace("prompt", "chain")
            self.chains[ch_name] = prompt_template | self.llm | parser_class()
            self.inputs[ch_name] = current_prompt["input_variables"]
        
        logging.info("–≠–∫–∑–µ–º–ø–ª—è—Ä –∫–ª–∞—Å—Å–∞ PortfolioAssistant —Å–æ–∑–¥–∞–Ω!")


    def _raise_error(self, msg: str):
        logging.error(msg)
        raise ValueError

    def _import_loader(self, loader_path: str) -> BaseLoader:
        """
        –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∑—á–∏–∫–æ–≤ langchain –ø–æ –ø–æ–ª–Ω–æ–º—É –ø—É—Ç–∏, 
        –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–ª–∞—Å—Å –∑–∞–≥—Ä—É–∑—á–∏–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω–æ–≥–æ –æ—Ç BaseLoader.
        """
        try:
            module_name, class_name = loader_path.rsplit(".", 1)
            module = importlib.import_module(module_name)
            loader_class = getattr(module, class_name)
            if not issubclass(loader_class, BaseLoader):
                msg = f"–ö–ª–∞—Å—Å {loader_class} –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ø–æ—Ç–æ–º–∫–æ–º BaseLoader"
                logging.error(msg)
                raise TypeError(msg)
            return loader_class
        except Exception as e:
            logging.error(
                f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ –∑–∞–≥—Ä—É–∑—á–∏–∫–∞ '{loader_path}': {e}")
            raise

    def _delete_dir(self, dir):
        if os.path.exists(dir):
            logging.warning(
                f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è '{dir}' —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –æ–Ω–∞ –±—É–¥–µ—Ç —É–¥–∞–ª–µ–Ω–∞")
            shutil.rmtree(dir)        

    def _download_repo_files(self):
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –∏–∑ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è —Å –ø–æ–º–æ—â—å—é GitHub API, 
        –∑–∞–≥—Ä—É–∂–∞–µ—Ç —Ñ–∞–π–ª—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é 'data_dir', 
        –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—É—é –≤ —Ñ–∞–π–ª–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é data). 

        –í–Ω–∏–º–∞–Ω–∏–µ! –ü—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ñ–∞–π–ª–æ–≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è —É–¥–∞–ª—è—é—Ç—Å—è –≤—Å—ë —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ 
        –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π 'data_dir' –∏ 'chroma_dir'.
        """
        logging.info("–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤ –∏–∑ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è")
        owner = self.config.get("owner", "rzarubayev")
        repo = self.config.get("repo", "machine-learning-portfolio")
        branch = self.config.get("branch", "main")

        file_types = tuple(self.loaders.keys())

        api_url = f"https://api.github.com/repos/{owner}/{repo}/git/trees/{branch}?recursive=1"

        logging.info(f"–ó–∞–ø—Ä–æ—Å —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è {api_url}")
        try:
            # –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤
            response = requests.get(api_url)
            response.raise_for_status()
            data = response.json()
            if data.get("truncated"):
                logging.warning("–í–Ω–∏–º–∞–Ω–∏–µ: —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –±—ã–ª —É—Å–µ—á–µ–Ω, "
                                "—Ç–∞–∫ –∫–∞–∫ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π")
            file_list = data.get("tree", [])
        except requests.exceptions.RequestException as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ GitHub API: {e}")
            return
        files = [
            f for f in file_list
            if f["type"] == "blob" and f["path"].endswith(file_types)
        ]
        if not files:
            logging.error(
                f"–í —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã —Ñ–∞–π–ª—ã —Å —Ä–∞—à–∏—Ä–µ–Ω–∏—è–º–∏ {file_types}")
            return
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π, –µ—Å–ª–∏ –æ–Ω–∏ —Å—É—â–µ—Å—Ç–≤—É—é—Ç
        output_dir = self.data_dir
        self._delete_dir(output_dir)
        chroma_dir = self.chroma_dir
        self._delete_dir(chroma_dir)

        for f in files:
            # –ü–æ–ª—É—á–∞–µ–º URL –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ —Å–∞–º–∏–º —Ñ–∞–π–ª–∞–º
            file_path = f["path"]
            raw_url = f"https://raw.githubusercontent.com/{owner}/{repo}/{branch}/{file_path}"
            # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫–∏, –µ—Å–ª–∏ –æ–Ω–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç
            local_file_path = os.path.join(output_dir, file_path)
            os.makedirs(os.path.dirname(local_file_path), exist_ok=True)
            # –°–∫–∞—á–∏–≤–∞–µ–º —Ñ–∞–π–ª—ã
            try:
                response = requests.get(raw_url)
                response.raise_for_status()
                with open(local_file_path, "w", encoding="utf8") as file:
                    file.write(response.text)
            except requests.exceptions.RequestException as e:
                logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ —Ñ–∞–π–ª–∞ {file_path}: {e}")

    def _load_documents(self) -> tuple[list, list]:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ —Ñ–∞–π–ª—ã –ø–æ—Å—Ä–µ–¥—Å—Ç–≤–æ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –∑–∞–≥—Ä—É–∑—á–∏–∫–æ–≤ langchain,
        –∏—Å–ø–æ–ª—å–∑—É—è —Å–ª–æ–≤–∞—Ä–∏ –¥–ª—è –≤—ã–±–æ—Ä–∞ –Ω—É–∂–Ω–æ–≥–æ –∑–∞–≥—Ä—É–∑—á–∏–∫–∞ –∏ –µ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.
        """
    
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
        general_docs, project_docs, target_files = [], [], []
        supported_ext = tuple(self.loaders.keys())

        # –ü–æ–ª—É—á–∏–º —Ç–æ–ª—å–∫–æ –∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Ñ–∞–π–ª—ã 
        for root, _, files in os.walk(self.data_dir):
            for f in files:
                if f.endswith(supported_ext):
                    target_files.append(os.path.join(root,f))

        if not target_files:
            logging.warning(f"–í –ø–∞–ø–∫–µ {self.data_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω—ã —Ñ–∞–π–ª—ã "
                            f"—Å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ–º {supported_ext}")
            return [], []

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤ –∏–∑ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        logging.info(
            f"–ù–∞–π–¥–µ–Ω–æ {len(target_files)} —Ñ–∞–π–ª–æ–≤. –ù–∞—á–∏–Ω–∞–µ—Ç—Å—è –æ–±—Ä–∞–±–æ—Ç–∫–∞.")
        for file_path in target_files:
            try:
                # –ü–æ–ª—É—á–µ–Ω–∏–µ –∑–∞–≥—Ä—É–∑—á–∏–∫–∞ –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é
                file_ext = os.path.splitext(file_path)[1]
                loader_cls = self.loaders[file_ext]

                if not loader_cls:
                    logging.warning("–ø—Ä–æ–ø—É—Å–∫ —Ñ–∞–π–ª–∞: –Ω–µ –Ω–∞–π–¥–µ–Ω –∑–∞–≥—Ä—É–∑—á–∏–∫ "
                                    f"–¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è {file_ext}")
                    continue

                params = self.loader_params.get(loader_cls.__name__, {})
                loader = loader_cls(file_path, **params)
                docs = loader.load()

                # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤ –æ–¥–∏–Ω –¥–æ–∫—É–º–µ–Ω—Ç
                content = "\n\n".join([doc.page_content for doc in docs])

                doc = Document(page_content=content)

                # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö (–∏—Å—Ç–æ—á–Ω–∏–∫, –∫–∞—Ç–µ–≥–æ—Ä–∏—è, –ø—Ä–æ–µ–∫—Ç)
                # –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π –ø–æ—Ä—Ç—Ñ–æ–ª–∏
                rel_path = os.path.relpath(file_path, self.data_dir)
                doc.metadata["source"] = rel_path

                parts = rel_path.split(os.sep)

                if len(parts) == 1:
                    # –§–∞–π–ª—ã –∏–∑ –∫–æ—Ä–Ω–µ–≤–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
                    general_docs.append(doc)
                else:
                    # –ü–æ–ª—É—á–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
                    doc.metadata["category"] = parts[0]
                    if len(parts) == 2:
                        # –ü—Ä–æ–µ–∫—Ç - –∏–º—è —Ñ–∞–π–ª–∞, –µ—Å–ª–∏ –Ω–µ—Ç —Å—É–±–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
                        doc.metadata["project"] = os.path.splitext(parts[1])[0]
                    else:
                        # –ï—Å–ª–∏ —ç—Ç–æ —Å—É–±–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è, –ø—Ä–æ–µ–∫—Ç - –µ–µ –∏–º—è
                        doc.metadata["project"] = parts[1]
                    project_docs.append(doc)

            except Exception as e:
                logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞ {file_path}: {e}")
        logging.info("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. "
                     f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(general_docs)} –æ–±—â–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ "
                     f"–∏ {len(project_docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –ø—Ä–æ–µ–∫—Ç–∞–º")
        return general_docs, project_docs

    def _limit_context(self, file_path_list: list, used_tokens = 0) -> str:
        """
        –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –Ω–∞ –¥–ª–∏–Ω—É –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. 
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç context_len –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤. 
        –î–ª—è –æ—Ü–µ–Ω–∫–∏ –¥–ª–∏–Ω—ã —Ç–æ–∫–µ–Ω–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è context_ratio, 
        –∫–æ—Ç–æ—Ä–∞—è –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤ –Ω–∞ —Ç–æ–∫–µ–Ω.

        –≠—Å–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–µ—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä–æ–∫—É '–ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ'.

        –ï—Å–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç—ã –µ—Å—Ç—å, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—ä–µ–¥–∏–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç, –∫–æ—Ç–æ—Ä—ã–π 
        –Ω–µ –≤—ã—Ö–æ–¥–∏—Ç –∑–∞ –ø—Ä–µ–¥–µ–ª—ã –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –ø–æ —Ç–æ–∫–µ–Ω–∞–º.
        """
        if len(file_path_list) == 0:
            return "–ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"
        result = []
        total_tokens = used_tokens
        for file_path in file_path_list:
            if file_path in self.project_map:
                tokens = len(self.project_map[file_path]) / self.context_ratio
                if total_tokens + tokens < self.context_len:
                    result.append(self.project_map[file_path])
                    total_tokens += tokens
                else:
                    break
            else:
                logging.warning(f"–§–∞–π–ª {file_path} –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö")

        return "\n\n".join(result)

    def run_chain(self, chain: str, prompt_inputs: dict) -> str | list | dict:
        """
        –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –æ–¥–Ω–æ–π —Ü–µ–ø–æ—á–∫–∏. 
        –ü—Ä–∏–Ω–∏–º–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ —Ü–µ–ø–æ—á–∫–∏ –∏ —Å–ª–æ–≤–∞—Ä—å —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏.
        """
        try:
            params = {key: prompt_inputs[key] for key in self.inputs[chain]}
            return self.chains[chain].invoke(params)
        except Exception as e:
            logging.error(
                f"–û—à–∏–±–∫–∞ –≤—ã–∑–æ–≤–∞ —Ñ—É–Ω–∫—Ü–∏–∏ 'run_chain' –¥–ª—è —Ü–µ–ø–æ—á–∫–∏ '{chain}: "
                f"{e}\n\n –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞:\n {prompt_inputs}")


    def query_portfolio(self, message, history) -> str:
        """
        –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –≤—Å–µ—Ö —Ü–µ–ø–æ—á–µ–∫ AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞, 
        –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –Ω–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –∏ –∏—Å—Ç–æ—Ä–∏—é —á–∞—Ç–∞.

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ç–≤–µ—Ç AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
        """
        logging.info("–ü–æ—Å—Ç—É–ø–∏–ª–æ –Ω–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ.")

        # –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è —Å–±–æ—Ä–∫–∏ –≤—Å–µ—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞
        inputs = {}
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –≤ —Ç–µ–∫—Å—Ç
        inputs["chat_history"] = "\n".join([
            f"**{msg['role']}:** {msg['content']}"
            for msg in history])
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        inputs["question"] = message

        # –ü–æ–ª—É—á–µ–Ω–∏–µ —É—Ç–æ—á–Ω–µ–Ω–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞ –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏ —á–∞—Ç–∞
        inputs["question"] = self.run_chain("history_chain", inputs)

        logging.info("–ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç –æ—Ç –ø–µ—Ä–≤–æ–π —Ü–µ–ø–æ—á–∫–∏.")
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∏ —Å–ø–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤
        inputs["general_context"] = self.general_context

        retrieved_chunks = self.retriever.invoke(inputs["question"])
        inputs["retrieved_chunks"] = "\n\n".join(
            [doc.page_content for doc in retrieved_chunks]
        )

        inputs["all_files"] = "\n".join(self.project_map.keys())

        # –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        files_to_read = self.run_chain("research_chain", inputs)
        logging.info(f"–ü–æ–ª—É—á–µ–Ω —Å–ø–∏—Å–æ–∫ –∏–∑ {len(files_to_read)} —Ñ–∞–π–ª–æ–≤ "
                     "–æ—Ç –≤—Ç–æ—Ä–æ–π —Ü–µ–ø–æ—á–∫–∏.")

        used_tokens = (
            len(inputs["question"]) +
            len(inputs["general_context"])) / self.context_ratio

        inputs["specific_content"] = self._limit_context(
            file_path_list=files_to_read,
            used_tokens=used_tokens
        )

        answer = self.run_chain("answer_chain", inputs)
        logging.info(f"–ü–æ–ª—É—á–µ–Ω –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç")
        return answer

if __name__ == "__main__":

    assistant = PortfolioAssistant()

    with gr.Blocks(title="AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ GitHub-–ø–æ—Ä—Ç—Ñ–æ–ª–∏–æ –ó–∞—Ä—É–±–∞–µ–≤–∞ –†—É—Å–ª–∞–Ω–∞",
                theme=gr.themes.Default()) as demo:
        gr.Markdown(
            """
            # ü§ñ AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ GitHub-–ø–æ—Ä—Ç—Ñ–æ–ª–∏–æ –ó–∞—Ä—É–±–∞–µ–≤–∞ –†—É—Å–ª–∞–Ω–∞
            –ó–∞–¥–∞–π—Ç–µ –º–Ω–µ –≤–æ–ø—Ä–æ—Å –æ –ø—Ä–æ–µ–∫—Ç–∞—Ö, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è—Ö –∏–ª–∏ –æ–ø—ã—Ç–µ –†—É—Å–ª–∞–Ω–∞.
            """
        )

        gr.ChatInterface(
            fn=assistant.query_portfolio,
            type="messages",
            chatbot=gr.Chatbot(type="messages"),
            examples=[
                "–í –∫–∞–∫–∏—Ö –ø—Ä–æ–µ–∫—Ç–∞—Ö –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è LightGBM?",
                "–ö–∞–∫–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å –¥–ª—è –¥–µ–ø–ª–æ—è –º–æ–¥–µ–ª–∏ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –∫–≤–∞—Ä—Ç–∏—Ä?",
                "–û–ø–∏—à–∏ –∑–∞–¥–∞—á—É –∏–∑ –ø—Ä–æ–µ–∫—Ç–∞ –ø–æ –∞–Ω–∞–ª–∏–∑—É —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤."
            ],
            textbox=gr.Textbox(
                placeholder="–ó–∞–¥–∞–π—Ç–µ –í–∞—à –≤–æ–ø—Ä–æ—Å –æ –ø—Ä–æ–µ–∫—Ç–∞—Ö...",
                container=True, scale=7
            )
        )

    demo.launch()