{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u36hVuZfpx3m"
      },
      "source": [
        "# AI-ассистент по моему GitHub портфолио\n",
        "\n",
        "**Цель:** Разработать MVP (прототип) чат-бота, который отвечает на вопросы о моем портфолио на GitHub, анализируя как общую, так и детальную информацию по проектам.\n",
        "\n",
        "**Задачи:**\n",
        "1. Подготовка и выбор стека\n",
        "2. Сбор и индексация данных\n",
        "3. Создание RAG-цепочки\n",
        "4. Подключение интерфейса и тестирование"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OmXhCeHs9aH"
      },
      "source": [
        "## Подготовка и выбор стека\n",
        "\n",
        "**Определение MVP:** MVP - это Python скрипт app.py, содержащий функцию `query_portfolio`, которая принимает на вход вопрос в виде текста и возвращает текстовый ответ, сгенерированный на основе текстового содержимого GitHub-портфолио.\n",
        "\n",
        "**Стек технологий:**\n",
        "- Оркестратор: LangChain - наиболее мощный и гибкий инструмент;\n",
        "- Векторная БД: ChromaDB - удобна для прототипирования;\n",
        "- Embeddings модель: all-MiniLM-L6-v2 - популярная, легкая и быстрая с хорошим балансом скорость/качество;\n",
        "- LLM: Google Gemini 2.5 Flash через API;\n",
        "- Веб-интерфейс: Gradio - для быстрого прототипирования."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpuv0Xy4fmq7"
      },
      "outputs": [],
      "source": [
        "!pip install -q google-ai-generativelanguage==0.6.15 \\\n",
        "langchain langchain-community langchain-huggingface \\\n",
        "langchain-google-genai langchain-unstructured langchain-chroma \\\n",
        "unstructured sentence-transformers jq gradio chromadb markdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "kEyi6gRXfAMB"
      },
      "outputs": [],
      "source": [
        "# Импорт библиотек\n",
        "import os\n",
        "import logging\n",
        "import requests\n",
        "\n",
        "import gradio as gr\n",
        "from tqdm import tqdm\n",
        "from langchain_community.document_loaders import (\n",
        "    UnstructuredMarkdownLoader, NotebookLoader,\n",
        "    TextLoader, PythonLoader, JSONLoader,\n",
        "\n",
        ")\n",
        "from langchain_unstructured.document_loaders import UnstructuredLoader\n",
        "from langchain.schema import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r61tVX3gyexN"
      },
      "outputs": [],
      "source": [
        "# Основные настройки\n",
        "OWNER = \"rzarubayev\"\n",
        "REPO = \"machine-learning-portfolio\"\n",
        "BRANCH = \"main\"\n",
        "\n",
        "# Исходные данные\n",
        "DATA_PATH = \"data\"\n",
        "# Путь к векторной базе\n",
        "CHROMA_DIR = \"chroma\"\n",
        "# Модель для эмбеддингов и LLM\n",
        "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "LLM_MODEL = \"gemini-2.5-flash\"\n",
        "\n",
        "# Используемые загрузчики и их параметры\n",
        "FILE_LOADERS = {\n",
        "    \".ipynb\": NotebookLoader,\n",
        "    \".md\": UnstructuredMarkdownLoader,\n",
        "    \".py\": PythonLoader,\n",
        "    \".txt\": TextLoader,\n",
        "    \".sh\": TextLoader,\n",
        "    \".yaml\": UnstructuredLoader,\n",
        "    \".yml\": UnstructuredLoader,\n",
        "    \".json\": JSONLoader\n",
        "}\n",
        "LOADER_PARAMS = {\n",
        "    \"NotebookLoader\": {\n",
        "        \"include_outputs\": False, # Не используем вывод ячеек\n",
        "        \"remove_newline\": True    # Убираем лишние переносы строк\n",
        "    },\n",
        "    \"UnstructuredLoader\": {\n",
        "        \"mode\": \"single\"\n",
        "    },\n",
        "    \"TextLoader\": {\n",
        "        \"encoding\": \"utf8\"\n",
        "    },\n",
        "    \"JSONLoader\": {\n",
        "        \"jq_schema\": \".\",\n",
        "        \"text_content\": False\n",
        "    },\n",
        "}\n",
        "\n",
        "# Установка уровня логирования в unstructured\n",
        "logging.getLogger(\"unstructured\").setLevel(logging.ERROR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZaNNrC_e1yW"
      },
      "source": [
        "**Настройка доступа к Gemini API:**\n",
        "Для получения доступа к API LLM Gemini 2.5 Flash необходимо сделать следующее:\n",
        "1. Зайти на сайт [Google AI studio](https://aistudio.google.com/apikey);\n",
        "2. Создать API ключ;\n",
        "3. Добавить ключ в переменную окружения GOOGLE_API_KEY (секрет в Hugging Face Spaces)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "E5gc-Ss_pNDO",
        "outputId": "0b279743-47fc-4d68-b1ee-6805f50f023e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'models/gemini-2.5-flash'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Проверка загрузки модели\n",
        "llm = ChatGoogleGenerativeAI(model=LLM_MODEL)\n",
        "llm.model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doYKeDfhpwIG"
      },
      "source": [
        "## Сбор и индексация данных\n",
        "\n",
        "Подготовим функции для загрузки списка файлов из GitHub, имеющих соответствующее расширение."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfaD_kvqwI4D"
      },
      "outputs": [],
      "source": [
        "# Фукнция для получения списка файлов из\n",
        "def download_repo_files(owner: str, repo: str, branch: str, file_types: tuple,\n",
        "                        output_dir: str):\n",
        "  \"\"\"\n",
        "  Получает список всех файлов во всех папках репозитория с помощью GitHub API,\n",
        "  загружает файлы в указанную директорию.\n",
        "  \"\"\"\n",
        "  api_url = f\"https://api.github.com/repos/{owner}/{repo}/git/trees/{branch}?recursive=1\"\n",
        "  print(f\"Запрашиваем структуру репозитория {api_url}\")\n",
        "  try:\n",
        "    # Получаем список файлов\n",
        "    response = requests.get(api_url)\n",
        "    response.raise_for_status()\n",
        "    data = response.json()\n",
        "    if data.get(\"truncated\"):\n",
        "      print(\"Внимание: список файлов был усечен, так как репозиторий слишком большой\")\n",
        "    file_list = data.get(\"tree\", [])\n",
        "  except requests.exceptions.RequestException as e:\n",
        "    print(f\"Ошибка при запросе к GitHub API: {e}\")\n",
        "    return\n",
        "  files = [\n",
        "      f for f in file_list\n",
        "      if f[\"type\"] == \"blob\" and f[\"path\"].endswith(file_types)\n",
        "  ]\n",
        "  if not files:\n",
        "    print(f\"В репозитории не найдены файлы с раширениями {file_types}\")\n",
        "    return\n",
        "\n",
        "  for f in tqdm(files, desc=\"Скачивание файлов\"):\n",
        "    # Получаем URL для доступа к самим файлам\n",
        "    file_path = f[\"path\"]\n",
        "    raw_url = f\"https://raw.githubusercontent.com/{owner}/{repo}/{branch}/{file_path}\"\n",
        "    # Создаем папки, если они отсутствуют\n",
        "    local_file_path = os.path.join(output_dir, file_path)\n",
        "    os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n",
        "    # Скачиваем файлы\n",
        "    try:\n",
        "      response = requests.get(raw_url)\n",
        "      response.raise_for_status()\n",
        "      with open(local_file_path, \"w\", encoding=\"utf8\") as file:\n",
        "        file.write(response.text)\n",
        "    except requests.exceptions.RequestException as e:\n",
        "      print(f\"Ошибка при скачивании файла {file_path}: {e}\")\n",
        "      tqdm.write(f\" - Ошибка при скачивании файла {file_path}: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysaOwEUpDjhC"
      },
      "source": [
        "Загрузим файлы в папку."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "742W-gx2Dx-N"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(DATA_PATH):\n",
        "  download_repo_files(OWNER, REPO, BRANCH, tuple(FILE_LOADERS.keys()), DATA_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_X1DnsrzED_4"
      },
      "source": [
        "Подготовим функцию для загрузки файлов, которая сформирует базу знаний для LLM (список документов LangChain)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "b48b0ad1"
      },
      "outputs": [],
      "source": [
        "def load_docs(data_path: str, loaders_map: dict,\n",
        "              params_map: dict) -> tuple[list, list]:\n",
        "  \"\"\"\n",
        "  Загружает все файлы посредством соответствующих загрузчиков langchain,\n",
        "  используя словари для выбора нужного загрузчика и его параметров.\n",
        "  \"\"\"\n",
        "  # Инициализируем переменные\n",
        "  general_docs, project_docs, target_files = [], [], []\n",
        "  supported_ext = tuple(loaders_map.keys())\n",
        "\n",
        "  # Получим только известные файлы (на всякий случай, если там что-то уже было)\n",
        "  for root, _, files in os.walk(data_path):\n",
        "    for f in files:\n",
        "      if f.endswith(supported_ext):\n",
        "        target_files.append(os.path.join(root,f))\n",
        "\n",
        "  if not target_files:\n",
        "    print(f\"В папке {data_path} не найдены файлы с расширением {supported_ext}\")\n",
        "    return [], []\n",
        "\n",
        "  # Обработка файлов\n",
        "  print(f\"Найдено {len(target_files)} файлов. Начинаем обработку...\")\n",
        "  for file_path in tqdm(target_files, desc=\"Обработка файлов\"):\n",
        "    try:\n",
        "      # Получение загрузчика по расширению\n",
        "      file_ext = os.path.splitext(file_path)[1]\n",
        "      loader_class = loaders_map[file_ext]\n",
        "\n",
        "      if not loader_class:\n",
        "        tqdm.write(\n",
        "          f\" - пропуск файла: не найден загрузчик для расширения {file_ext}\")\n",
        "        continue\n",
        "\n",
        "      loader_params = params_map.get(loader_class.__name__, {})\n",
        "      loader = loader_class(file_path, **loader_params)\n",
        "      docs = loader.load()\n",
        "      # Объединение в один документ\n",
        "      content = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "      doc = Document(page_content=content)\n",
        "      # Добавление метаданных (источник, категория, проект)\n",
        "      # в соответствии со структурой портфолио\n",
        "      rel_path = os.path.relpath(file_path, data_path)\n",
        "      doc.metadata[\"source\"] = rel_path\n",
        "\n",
        "      parts = rel_path.split(os.sep)\n",
        "\n",
        "      if len(parts) == 1:\n",
        "        # Файлы из корневой директории\n",
        "        doc.metadata[\"category\"] = \"general\"\n",
        "        general_docs.append(doc)\n",
        "      else:\n",
        "        # Получаем категорию по названию директории\n",
        "        doc.metadata[\"category\"] = parts[0]\n",
        "        if len(parts) == 2:\n",
        "          # Проект - имя файла, если нет субдиректорий\n",
        "          doc.metadata[\"project\"] = os.path.splitext(parts[1])[0]\n",
        "        else:\n",
        "          # Если это субдиректория, проект - ее имя\n",
        "          doc.metadata[\"project\"] = parts[1]\n",
        "        project_docs.append(doc)\n",
        "\n",
        "    except Exception as e:\n",
        "      tqdm.write(f\" - Ошибка при обработке файла {file_path}: {e}\")\n",
        "\n",
        "  return general_docs, project_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AP1tBLnKree9"
      },
      "source": [
        "Функция готова, получим список документов LangChain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erz5SilQrl-r",
        "outputId": "326047ed-40b9-4bf9-e9ff-02fed6b77077"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Найдено 125 файлов. Начинаем обработку...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Обработка файлов: 100%|██████████| 125/125 [00:12<00:00,  9.81it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Количество общих документов: 2\n",
            "Количество проектных документов: 123\n",
            "Пример документа: \n",
            "{\"id\": 116938, \"rooms\": 1, \"total_area\": 32.5999984741, \"kitchen_area\": 6, \"living_area\": 18.7000007629, \"floor\": 9, \"studio\": false, \"is_apartment\": false, \"building_type_int\": 4, \"build_year\": 1970,\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "general, projects = load_docs(DATA_PATH, FILE_LOADERS, LOADER_PARAMS)\n",
        "sample_doc = projects[7]\n",
        "print()\n",
        "print(f\"Количество общих документов: {len(general)}\")\n",
        "print(f\"Количество проектных документов: {len(projects)}\")\n",
        "print(f\"Пример документа: \\n{sample_doc.page_content[:200]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_wfqeTWrmHr"
      },
      "source": [
        "Документы загружены, для подачи их в LLM в качестве контекста, необходимо нарезать их на более мелкие, семантически связанные части — чанки. Это важный шаг, так как поиск по небольшим чанкам работает гораздо точнее, а также у языковых моделей есть ограничение на размер входного контекста. Мы будем использовать RecursiveCharacterTextSplitter, который интеллектуально разбивает текст, с двумя ключевыми параметрами: chunk_size=1000 — оптимальный размер одного чанка в символах для сохранения контекста, и chunk_overlap=200 — пересечение между чанками, чтобы не терять смысл на их стыках.\n",
        "\n",
        "Применим гибридный подход к формированию контекста: общие документы будем подавать в модель целиком для сохранения глобальной информации, а проектные файлы нарежем на чанки. Это позволит ретриверу находить наиболее релевантные фрагменты для ответа на конкретные вопросы."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rE5QeO1r-5d",
        "outputId": "13b177b2-6bf3-43a4-dff6-d70c1b44e2c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Количество чанков: 2994\n",
            "Пример чанка: \n",
            "./services/stop_compose.sh\n",
            "\n",
            "Либо можно остановить его вручную из папки services\\:\n",
            "\n",
            "cd services/\n",
            "docker compose stop\n",
            "\n",
            "4. Скрипт симуляции нагрузки\n",
            "\n",
            "Скрипт генерирует 600 запросов в течение ~150 секунд.\n"
          ]
        }
      ],
      "source": [
        "if projects:\n",
        "  text_splitter = RecursiveCharacterTextSplitter(\n",
        "      chunk_size=1000,\n",
        "      chunk_overlap=200\n",
        "  )\n",
        "  chunks = text_splitter.split_documents(projects)\n",
        "  print(f\"Количество чанков: {len(chunks)}\")\n",
        "  print(f\"Пример чанка: \\n{chunks[37].page_content[:200]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXmN30Ctr_BU"
      },
      "source": [
        "Следующий шаг — создание векторной базы знаний. Мы инициализируем модель эмбеддингов, которая преобразует каждый текстовый чанк в числовой вектор и сохраним на диск векторное хранилище из этих чанков с помощью ChromaDB. Этот процесс называется индексацией и позволит выполнять быстрый семантический поиск по нашей базе знаний."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7rqrpk1sOT-",
        "outputId": "57e5cb97-1ed0-47f8-c4f4-2e7d97c8dfa3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Количество векторов: 2994\n"
          ]
        }
      ],
      "source": [
        "if chunks:\n",
        "  embedding = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)\n",
        "  if os.path.exists(CHROMA_DIR):\n",
        "    # Загружаем базу, если она существует\n",
        "    vector_store = Chroma(\n",
        "        persist_directory=CHROMA_DIR,\n",
        "        embedding_function=embedding\n",
        "    )\n",
        "  else:\n",
        "    # Создаем базу, если ее нет\n",
        "    vector_store = Chroma.from_documents(\n",
        "        documents=chunks,\n",
        "        embedding=embedding,\n",
        "        persist_directory=CHROMA_DIR\n",
        "    )\n",
        "  print(f\"Количество векторов: {vector_store._collection.count()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWp-P2m0sZp3"
      },
      "source": [
        "Наша база знаний создана и векторизована, теперь нам нужен инструмент, который сможет эффективно извлекать из нее релевантную информацию. В LangChain эту роль выполняет Ретривер (Retriever). Мы создадим его из нашего векторного хранилища и настроим так, чтобы по любому запросу он находил 15 наиболее семантически близких документа (k=15), которые затем будут переданы в качестве контекста для LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhRLSPGAsdCH",
        "outputId": "c85a1982-7209-4d70-81a3-f15c9012dd12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Количество найденных документов: 15\n",
            "Пример документа: \n",
            "В целом поставленную задачу можно интерпретировать как задачу многометочной (multi-label) классификации, в том числе возможно разернуть продукты по клиентам а также дополнить признаками по продукту, н\n"
          ]
        }
      ],
      "source": [
        "if vector_store:\n",
        "  retriever = vector_store.as_retriever(\n",
        "      search_type=\"similarity\",\n",
        "      search_kwargs={\"k\": 15}\n",
        "  )\n",
        "\n",
        "  test_query = \"Какой имеется опыт с multi-label классификацией\"\n",
        "  rel_docs = retriever.invoke(test_query)\n",
        "  print(f\"Количество найденных документов: {len(rel_docs)}\")\n",
        "  print(f\"Пример документа: \\n{rel_docs[0].page_content[:200]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2vMj-s7sOcV"
      },
      "source": [
        "## Создание RAG цепочки"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-10gRieUnCMT"
      },
      "source": [
        "Чтобы сделать нашего ассистента умнее, реализуем более сложную, трехэтапную архитектуру. Вместо того чтобы слепо доверять результатам векторного поиска, сначала заставим модель подумать. На первом этапе модель суммаризует историю переписки оставив максимально релевантный контент к вопросу пользователя. На втором этапе модель будет работать как исследователь, который определит наиболее релевантные файлы проектов. И только последняя модель \"ассистент\" будет формировать финальный ответ на основе всего этого расширенного контекста.\n",
        "\n",
        "Начнем с создания \"мозга\" для суммаризации истории переписки — подготовим для него специальный промпт."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "hyn5DYbccC1o"
      },
      "outputs": [],
      "source": [
        "HISTORY_PROMPT_TEMPLATE = \"\"\"\n",
        "Твоя задача — проанализировать историю диалога и новый вопрос пользователя,\n",
        "а затем создать краткую и самодостаточную сводку.\n",
        "Эта сводка будет использована для подготовки ответа другой AI модели.\n",
        "\n",
        "Внимательно изучи историю и вопрос. Затем сделай ОДНО из двух:\n",
        "1. **Переформулируй вопрос:** если новый вопрос пользователя зависит от\n",
        "контекста (например, содержит ссылку на предыдущий вопрос или ответ,\n",
        "со словами: \"это\", \"там\", \"а в нем\" и аналогичные), перепиши его так,\n",
        "чтобы он был понятен без истории чата как контекст для модели.\n",
        "2. **Создай резюме:** если переформулировать вопрос сложно,\n",
        "напиши очень короткое (2-3 предложения) ключевой информации из истории,\n",
        "необходимой для ответа на новый вопрос.\n",
        "В конце добавь новый вопрос пользователя.\n",
        "\n",
        "История переписки:\n",
        "{chat_history}\n",
        "Новый вопрос:\n",
        "{question}\n",
        "Сжатый контекст:\n",
        "\"\"\"\n",
        "\n",
        "history_prompt = PromptTemplate(\n",
        "    template=HISTORY_PROMPT_TEMPLATE,\n",
        "    input_variables=[\"chat_history\", \"question\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYsSFB4qcChl"
      },
      "source": [
        "Подготовим промп для \"исследователя\". Для него подаем общую информацию, вопрос пользователя, найденные ретривером релевантные чанки и полный список всех файлов в нашей базе знаний. Ее задача — не отвечать на вопрос, а проанализировать все вводные данные и решить, какие документы нужно прочитать целиком для наиболее полного ответа."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "R46e2qc-dLHz"
      },
      "outputs": [],
      "source": [
        "RESEARCH_PROMPT_TEMPLATE = \"\"\"\n",
        "Ты умный AI-исследователь. Твоя задача - помочь ответить на вопрос пользователя.\n",
        "Ниже представлены общие сведения по проектам и владельце портфолио, сам вопрос,\n",
        "несколько релевантных фрагментов, найденных в базе знаний и полный список всех\n",
        "доступных файлов проектов.\n",
        "\n",
        "Проанализируй эту информацию и реши, какие файлы из полного списка нужно\n",
        "прочитать ЦЕЛИКОМ, чтобы дать наиболее полный и точный ответ.\n",
        "Учитывай и те файлы, из которых взяты фаргменты, и любые другие,\n",
        "которые кажутся тебе релевантными. Не добавляй файлы, которые не считаешь\n",
        "релевантными. Составь список по уменьшению релевантности.\n",
        "\n",
        "Твой ответ должен быть ТОЛЬКО списком путей к файлам в формате чистого JSON.\n",
        "Никаких лишних слов и кавычек.\n",
        "Если не найдешь ничего подходящего, верни пустой список - []\n",
        "\n",
        "--- Общая информация о проектах и владельце портфолио ---\n",
        "{general_context}\n",
        "\n",
        "--- Вопрос пользователя ---\n",
        "{question}\n",
        "\n",
        "--- Найденные фрагменты (подсказки) ---\n",
        "{retrieved_chunks}\n",
        "\n",
        "--- Список всех доступных файлов ---\n",
        "{all_files}\n",
        "\n",
        "--- Ответ в формате JSON ---\n",
        "\"\"\"\n",
        "\n",
        "research_prompt = PromptTemplate(\n",
        "    template=RESEARCH_PROMPT_TEMPLATE,\n",
        "    input_variables=[\n",
        "        \"general_context\", \"question\",\n",
        "        \"retrieved_chunks\", \"all_files\"\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IUvZhSesdJG"
      },
      "source": [
        "Теперь, когда у нас есть промпт \"исследователя\", по которому модель способна определить наиболее релевантные файлы, подготовим основной промпт для нашей модели. Эта модель будет играть роль AI-ассистента. В ее задачу входит синтезировать исчерпывающий и связный ответ для пользователя, опираясь на два источника: постоянный общий контекст и динамический контекст, состоящий из полного содержимого файлов, которые на предыдущем шаге порекомендовала \"исследовательская\" модель."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "jW-c2ZGmsiu9"
      },
      "outputs": [],
      "source": [
        "ANSWER_PROMPT_TEMPLATE = \"\"\"\n",
        "Ты - AI-ассистент, который помогает узнать о проектах и опыте Зарубаева Руслана.\n",
        "Отвечай на вопрос пользователя, опираясь на ВОПРОС С КОНТЕКСТОМ,\n",
        "ОБЩИЙ КОНТЕКСТ и на КОНТЕКСТ ПО ВОПРОСУ.\n",
        "Правила ответа:\n",
        "1. Если вопрос не касается проектов, опыта или компетенций Руслана,\n",
        "вежливо сообщи, что можешь отвечать только на вопросы,\n",
        "связанные с его портфолио;\n",
        "2. Если в предоставленном контексте нет информации для ответа на вопрос,\n",
        "честно сообщи об этом;\n",
        "3. Не придумывай информацию.\n",
        "\n",
        "--- ВОПРОС С КОНТЕКСТОМ ---\n",
        "{question}\n",
        "--- КОНЕЦ ВОПРОСА С КОНТЕКСТОМ ---\n",
        "\n",
        "--- ОБЩИЙ КОНТЕКСТ (информация о Руслане и его проектах в портфолио) ---\n",
        "{general_context}\n",
        "--- КОНЕЦ ОБЩЕГО КОНТЕКСТА ---\n",
        "\n",
        "--- КОНТЕКСТ ПО ВОПРОСУ (содержимое рекомендованных файлов) ---\n",
        "{specific_content}\n",
        "--- КОНЕЦ КОНТЕКСТА ПО ВОПРОСУ ---\n",
        "\n",
        "Ответ:\n",
        "\"\"\"\n",
        "\n",
        "answer_prompt = PromptTemplate(\n",
        "    template=ANSWER_PROMPT_TEMPLATE,\n",
        "    input_variables=[\n",
        "        \"question\", \"general_context\",\n",
        "        \"specific_content\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7_E6-y5wS_o"
      },
      "source": [
        "Подготовим общий контекст и карту всех проектных документов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "7x6fnaW3wm_g"
      },
      "outputs": [],
      "source": [
        "# Общий контекст\n",
        "general_context = \"\\n\\n\".join([doc.page_content for doc in general])\n",
        "# карта проектных документов\n",
        "project_map = {doc.metadata[\"source\"]: doc.page_content for doc in projects}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIfyTxYUeo_S"
      },
      "source": [
        "Подготовим функции для всех цепочек:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21Cq0LHwlx3O"
      },
      "outputs": [],
      "source": [
        "# Функция для цепочки суммаризатора истории переписки\n",
        "def run_history_chain(prompt: PromptTemplate, chat_history: list,\n",
        "                      query: str, llm_model) -> str:\n",
        "  \"\"\"\n",
        "  Запускает цепочку суммирования истории переписки для получения ответа\n",
        "  \"\"\"\n",
        "  history_chain = prompt | llm_model | StrOutputParser()\n",
        "  return history_chain.invoke({\n",
        "      \"chat_history\": chat_history,\n",
        "      \"question\": query\n",
        "  })\n",
        "\n",
        "# Функция для цепочки исследователя\n",
        "def run_research_chain(prompt: PromptTemplate, query: str, general_context: str,\n",
        "                       project_map: dict, llm_model, retriever) -> list:\n",
        "  \"\"\"\n",
        "  Запускает цепочку исследователя для получения списка файлов\n",
        "  \"\"\"\n",
        "  # Подготовка переменных для шаблона\n",
        "  retrieved_chunks = retriever.invoke(query)\n",
        "  retrieved_chunks = \"\\n\\n\".join([doc.page_content for doc in retrieved_chunks])\n",
        "  all_files = \"\\n\".join(project_map.keys())\n",
        "  # Создание цепочки\n",
        "  research_chain = prompt | llm_model | JsonOutputParser()\n",
        "  # Запуск цепочки\n",
        "  try:\n",
        "    result = research_chain.invoke({\n",
        "        \"general_context\": general_context,\n",
        "        \"question\": query,\n",
        "        \"retrieved_chunks\": retrieved_chunks,\n",
        "        \"all_files\": all_files\n",
        "    })\n",
        "    print(f\"Найдено файлов: {len(result)}\")\n",
        "    return result\n",
        "  except Exception as e:\n",
        "    print(\"Ошибка парсинга ответа модели: \", e)\n",
        "    return []\n",
        "\n",
        "# Функция для цепочки ассистента\n",
        "def run_answer_chain(prompt: PromptTemplate, query: str, general_context: str,\n",
        "                     project_map: dict, files_to_read: list, llm_model,\n",
        "                     context_length = 100000, context_ratio = 2.5) -> str:\n",
        "  \"\"\"\n",
        "  Запускает цепочку ассистента для получения ответа\n",
        "  \"\"\"\n",
        "  # Получение списка файлов\n",
        "  if len(files_to_read) == 0:\n",
        "    specific_content = \"Ничего не найдено\"\n",
        "  else:\n",
        "    # Проверка ограничения по количеству токенов\n",
        "    docs = []\n",
        "    total_tokens = (len(query) + len(general_context)) / context_ratio\n",
        "    for file in files_to_read:\n",
        "      # Проверка документа на существование в списке\n",
        "      if file in project_map:\n",
        "        doc_tokens = len(project_map[file]) / context_ratio\n",
        "        if total_tokens + doc_tokens < context_length:\n",
        "          docs.append(project_map[file])\n",
        "          total_tokens += doc_tokens\n",
        "        else:\n",
        "          break\n",
        "      else:\n",
        "          print(f\"Файл '{file}' не найден в project_map\")\n",
        "    specific_content = \"\\n\\n\".join(docs)\n",
        "  # Создание цепочки\n",
        "  answer_chain = prompt | llm_model | StrOutputParser()\n",
        "  # Запуск цепочки\n",
        "  return answer_chain.invoke({\n",
        "      \"general_context\": general_context,\n",
        "      \"specific_content\": specific_content,\n",
        "      \"question\": query\n",
        "  })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Hra5dY2s1up"
      },
      "source": [
        "## Подключение интерфейса и тестирование"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aM7fwckMsi24"
      },
      "source": [
        "Функции для каждой цепочки подготовлены, объединим их в одну функцию, которая получает на вход запрос пользователя и возвращает ответ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "IUtxmcVW5Kyg"
      },
      "outputs": [],
      "source": [
        "# Основная функция\n",
        "def get_bot_response(message: str, history: list) -> str:\n",
        "  \"\"\"\n",
        "  Получает на вход вопрос пользователя и историю, возвращает ответ.\n",
        "  \"\"\"\n",
        "  # Преобразование истории в текст\n",
        "  chat_history = \"\\n\".join([\n",
        "      f\"**{msg['role']}:** {msg['content']}\"\n",
        "      for msg in history])\n",
        "  # Получение вопроса с контекстом\n",
        "  question = run_history_chain(\n",
        "      history_prompt, chat_history, message, llm\n",
        "  )\n",
        "\n",
        "  files_to_read = run_research_chain(\n",
        "      research_prompt, question, general_context,\n",
        "      project_map, llm, retriever\n",
        "  )\n",
        "\n",
        "  answer = run_answer_chain(\n",
        "      answer_prompt, question, general_context,\n",
        "      project_map, files_to_read, llm\n",
        "  )\n",
        "  return answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLiPNoVq77Qj"
      },
      "source": [
        "Проверим работу функций"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLQRP2OL7-Od",
        "outputId": "e615e10f-2db6-4be3-e0ab-4fe92e480d3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Найдено файлов: 3\n",
            "LightGBM используется в следующих проектах Зарубаева Руслана:\n",
            "\n",
            "1.  **Стоимость автомобилей**: Проект по регрессии, где LightGBM применялся для определения рыночной стоимости автомобилей.\n",
            "2.  **Заказы такси**: Проект по прогнозированию временных рядов, где LightGBM использовался для предсказания количества заказов такси.\n",
            "3.  **Токсичность текста**: Проект по классификации, где LightGBM применялся для определения токсичности комментариев.\n"
          ]
        }
      ],
      "source": [
        "test_query = \"В каких проектах используется LightGBM\"\n",
        "print(get_bot_response(test_query, []))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79bGDG3ss7aR"
      },
      "source": [
        "Основная функция-обработчик подготовлена, обернем ее в gradio.ChatInterface. Это автоматически создаст готовый веб-интерфейс чат-бота."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "id": "t36rtR3195sK",
        "outputId": "f3a85cf3-c930-4aae-87d1-acf5d1f3ed9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://3c40078f8275769c41.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://3c40078f8275769c41.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Найдено файлов: 3\n"
          ]
        }
      ],
      "source": [
        "# Описываем с помощью блоков для настройки\n",
        "with gr.Blocks(title=\"AI-ассистент по GitHub-портфолио Зарубаева Руслана\",\n",
        "               theme=gr.themes.Default()) as demo:\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "# 🤖 AI-ассистент по GitHub-портфолио Зарубаева Руслана\n",
        "Задайте мне вопрос о проектах, использованных технологиях или опыте Руслана.\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    gr.ChatInterface(\n",
        "        fn=get_bot_response,\n",
        "        type=\"messages\",\n",
        "        chatbot=gr.Chatbot(type=\"messages\"),\n",
        "        examples=[\n",
        "            \"В каких проектах использовался LightGBM?\",\n",
        "            \"Какие инструменты использовались для деплоя модели стоимости квартир?\",\n",
        "            \"Опиши задачу из проекта по анализу токсичности комментариев.\"\n",
        "        ],\n",
        "        textbox=gr.Textbox(\n",
        "            placeholder=\"Задайте Ваш вопрос о проектах...\",\n",
        "            container=True, scale=7\n",
        "        )\n",
        "    )\n",
        "\n",
        "demo.launch(debug=True, share=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ae2Ayc0rrbP"
      },
      "source": [
        "## Итоги проекта\n",
        "\n",
        "В рамках данного pet-проекта была успешно решена задача создания интеллектуального чат-бота, способного вести диалог и отвечать на вопросы по содержимому GitHub-репозитория с учебными проектами.\n",
        "\n",
        "Основной целью было создание не простого RAG-бота, а продвинутого диалогового ассистента, который:\n",
        "1. Понимает контекст беседы, используя историю предыдущих сообщений.\n",
        "2. Анализирует большой объем информации, включая множество ipynb и md файлов.\n",
        "3. Предоставляет точные ответы, основанные на содержимом файлов, а не на общих знаниях.\n",
        "4. Имеет удобный веб-интерфейс для демонстрации и взаимодействия.\n",
        "\n",
        "### ⚙️ Архитектура решения\n",
        "\n",
        "Для достижения поставленной цели была спроектирована и реализована продвинутая трехэтапная RAG-цепочка:\n",
        "\n",
        "1. **Этап 1:** Создание самодостаточного запроса.\n",
        "  - На этом этапе специальная LLM-цепочка анализирует историю диалога и новый вопрос пользователя.\n",
        "  - Генерируется \"сжатый\" самодостаточный запрос — либо переформулированный вопрос, либо краткое резюме диалога с добавлением нового вопроса. Это позволяет избавиться от необходимости передавать всю историю на следующие этапы.\n",
        "\n",
        "2. **Этап 2:** Поиск релевантных файлов.\n",
        "  - Самодостаточный запрос с первого этапа используется для поиска наиболее релевантных документов (чанков) в векторной базе данных ChromaDB.\n",
        "  - На основе найденных чанков и общего списка файлов вторая LLM-цепочка (\"исследователь\") формирует список полных путей к файлам, которые необходимо прочитать для исчерпывающего ответа.\n",
        "\n",
        "3. **Этап 3:** Генерация финального ответа.\n",
        "  - Третья LLM-цепочка (\"ответчик\") получает на вход самодостаточный запрос, общую информацию о портфолио и полное содержимое файлов, рекомендованных \"исследователем\".\n",
        "  - На основе этих данных генерируется финальный, развернутый ответ для пользователя с учетом всех правил (ответы только по теме, запрет на выдумывание информации).\n",
        "\n",
        "## 🛠️ Ключевые технологии\n",
        "**Оркестрация:** LangChain\n",
        "\n",
        "**LLM:** Google Gemini 2.5 Flash\n",
        "\n",
        "**Векторная БД:** ChromaDB\n",
        "\n",
        "**Эмбеддинги:** sentence-transformers/all-MiniLM-L6-v2\n",
        "\n",
        "**Интерфейс:** Gradio\n",
        "\n",
        "## ✅ Результат\n",
        "Результатом работы является полнофункциональный прототип чат-бота, протестированный в данном ноутбуке. Он успешно справляется с диалоговыми сценариями, корректно использует контекст беседы и предоставляет точные ответы на основе загруженных документов.\n",
        "\n",
        "## 🚀 Дальнейшие шаги\n",
        "Следующим шагом является упаковка разработанного решения в веб-приложение и его деплой на платформе Hugging Face Spaces для публичной демонстрации. Это позволит добавить прямую ссылку на работающего ассистента в портфолио."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
